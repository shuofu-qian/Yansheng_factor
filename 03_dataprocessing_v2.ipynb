{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from scipy.stats.mstats import winsorize\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureDataFrame:\n",
    "    def __init__(self,data:pd.DataFrame):\n",
    "        self.data = data\n",
    "\n",
    "    def filter_asset(self,asset_pool:pd.DataFrame):\n",
    "        \"\"\"Filter the asset in self using the data in asset_pool\n",
    "        \n",
    "        asset_pool:     A dataframe whose first column is asset code\n",
    "        \"\"\"\n",
    "\n",
    "        asset_pool = asset_pool.iloc[:,0:1].rename(columns = {asset_pool.columns[0]:\"asset\"})\n",
    "        asset_pool['asset'] = asset_pool['asset'].apply(lambda x: str(x))\n",
    "        self.data = pd.merge(self.data,asset_pool,how='inner',on = 'asset').sort_values(['asset','datetime'])\n",
    "\n",
    "        return self\n",
    "\n",
    "    def fill_na(self):\n",
    "        self.data = self.data.sort_values(by=['datetime','asset'])\n",
    "        self.data = self.data.groupby(['asset']).apply(lambda x: x.fillna(method='ffill').fillna(method='bfill'))\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def drop_samevalue(self):\n",
    "        \"\"\"Drop the column from the third one if all the data in this column is the same or all of them is NaN\"\"\"\n",
    "\n",
    "        drop_list = [column for column in self.data.columns[2:] if not np.nanstd(self.data[column]) > 0]\n",
    "        self.data = self.data.drop(columns = drop_list)\n",
    "\n",
    "        return self\n",
    "\n",
    "\n",
    "    def __unstack(self):\n",
    "        try:\n",
    "            self.data['minute'] = self.data['datetime'].apply(lambda x: x[-8:])\n",
    "            self.data['datetime'] = self.data['datetime'].apply(lambda x: x[:10])\n",
    "        except:\n",
    "            raise Exception(\"The datatime column is not suitable to be unstacked or the data has reached the required shape\")\n",
    "\n",
    "        self.data = self.data.sort_values(['asset','datetime','minute']).set_index(['asset','datetime','minute']).unstack(2)\n",
    "        self.data.columns = [\"_\".join(tuple) for tuple in self.data.columns]\n",
    "        self.data.reset_index(inplace=True)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def __stack(self):\n",
    "        try:\n",
    "            self.data.set_index([\"asset\",\"datetime\"],inplace=True)\n",
    "            column_list = [\"asset\",\"datetime\"] + list(dict.fromkeys([column[:-9] for column in self.data.columns]))\n",
    "            self.data.columns = pd.MultiIndex.from_tuples([(column[:-9],column[-8:]) for column in self.data.columns],names = (\"\",\"minute\"))\n",
    "            self.data = self.data.stack(1).reset_index()\n",
    "        except:\n",
    "            raise Exception(\"The column nams are not suitable to be stacked or the data has reached the required shape\")\n",
    "\n",
    "        self.data[\"datetime\"] = self.data[\"datetime\"].str.cat(self.data[\"minute\"],sep = \" \")\n",
    "        self.data = self.data.drop(columns=['minute'])[column_list]\n",
    "\n",
    "        return self\n",
    "\n",
    "    def change_shape(self,unstack:bool=True):\n",
    "        \"\"\"Suppose there are n stocks, s time intervals in one day, m features,\n",
    "        change the data shape from (n*s,m) to (n,m*s) if unstack == True, otherwise the opposite\"\"\"\n",
    "        \n",
    "        if unstack:\n",
    "            self.__unstack()\n",
    "        else:\n",
    "            self.__stack()\n",
    "            \n",
    "        return self\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelDataFrame:\n",
    "    def __init__(self,data:pd.DataFrame):\n",
    "        self.data = data\n",
    "    \n",
    "    def get_return(self, adj = False, return_frequency = \"1d\", close_open = False):\n",
    "        \"\"\"Sort the data by code and time, append a new column naming 'return' in the last with NaN being filled with 0\n",
    "        \n",
    "        adj:                    whether to calculate using adj price, default == False\n",
    "        return_frequency:       the lenth of time when calculating return, can be chosen from '1d','2d','4m' etc. default =='1d'\n",
    "        close_open:             whether to calculate return using today's close price and open price\n",
    "        \"\"\"\n",
    "\n",
    "        adj_str = adj and \"adj_\" or \"\"                          \n",
    "        self.data.sort_values([\"symbol_id\",\"trade_date\"],inplace=True)\n",
    "        fre_num = int(return_frequency[:-1])\n",
    "\n",
    "        if close_open:\n",
    "            self.data[\"return\"] = np.log( self.data[adj_str+\"close_price\"]/self.data[adj_str+\"open_price\"] )\n",
    "        else:\n",
    "            if return_frequency[-1] == \"d\":\n",
    "                self.data[\"return\"] = np.log( self.data[[\"symbol_id\",adj_str+\"close_price\"]].groupby(\"symbol_id\")[adj_str+\"close_price\"].shift(-fre_num)/\\\n",
    "                                              self.data[[\"symbol_id\",adj_str+\"close_price\"]].groupby(\"symbol_id\")[adj_str+\"close_price\"].shift(0) )\n",
    "            elif return_frequency[-1] == \"m\":\n",
    "                self.data[\"year_month\"] = self.data[\"trade_date\"].apply(lambda x: x[0:6])\n",
    "                temp_df = self.data[[\"symbol_id\",adj_str+\"close_price\",\"year_month\"]].groupby([\"symbol_id\",\"year_month\"]).last().reset_index()\n",
    "                temp_df[\"return\"] = np.log( temp_df.groupby(\"symbol_id\")[adj_str+\"close_price\"].shift(-fre_num)/\\\n",
    "                                            temp_df.groupby(\"symbol_id\")[adj_str+\"close_price\"].shift(0) )\n",
    "                self.data = pd.merge(self.data,temp_df[[\"symbol_id\",\"year_month\",\"return\"]], how = \"left\", on = [\"symbol_id\",\"year_month\"])          \n",
    "                self.data = self.data.drop(columns = [\"year_month\"]) \n",
    "            else:\n",
    "                raise Exception(\"The value of return_frequency is set wrong\")                   \n",
    "\n",
    "        self.data[\"return\"] = self.data[\"return\"].fillna(0)\n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProcessArray:\n",
    "    def __init__(self,data:pd.DataFrame or np.ndarray):\n",
    "        self.array = np.array(data)\n",
    "\n",
    "    def __call__(self):\n",
    "        return self.array\n",
    "\n",
    "    # Actually, when there is no extreme value, e.g.[0,1,1,1,1,1,1], using median to transform will get [1,1,1,1,1,1,1]\n",
    "    # But it is not advisable for the std after transform is zero not one!\n",
    "\n",
    "    # Solution one: transform the extreme value according to their percentile\n",
    "    # Solution two: use (0.5,99.5) data to calculate mean and std\n",
    "\n",
    "    def fill_na(self,strategy='mean',fill_value=None, keep_empty_features=True):\n",
    "        \"\"\"strategy:'mean','median','most_frequency','constant'\"\"\"\n",
    "\n",
    "        transformer = SimpleImputer(strategy=strategy, fill_value=fill_value, keep_empty_features=keep_empty_features)\n",
    "        self.array = transformer.fit(self.array).transform(self.array)\n",
    "\n",
    "        return self\n",
    "\n",
    "\n",
    "    def get_zscore(self,limit_extreme_value=True,limits=[0.005,0.005],robust=False,feature_range=(25,75),std_times=3):\n",
    "        if limit_extreme_value:\n",
    "            self.array = winsorize(self.array,limits=limits,axis=0).data\n",
    "\n",
    "            # median = np.nanmedian(self.array,axis=0)\n",
    "            # median_std = np.nanmedian(np.abs(self.array-median),axis=0)\n",
    "            # self.array = np.clip(self.array,median-std_times*median_std,median+std_times*median_std)\n",
    "\n",
    "            # mean = np.nanmean(self.array,axis=0)\n",
    "            # mean_std = np.nanstd(self.array,axis=0)\n",
    "            # self.array = np.clip(self.array,mean-std_times*mean_std,mean+std_times*mean_std)\n",
    "\n",
    "        if robust:\n",
    "            self.array = preprocessing.robust_scale(self.array,feature_range=feature_range)\n",
    "        else:\n",
    "            self.array = preprocessing.scale(self.array)\n",
    "\n",
    "        return self\n",
    "\n",
    "\n",
    "\n",
    "# 3倍上下限限制极值,z-score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_feature_label(df_feature,df_label) -> pd.DataFrame:\n",
    "    df_feature['trade_date'] = df_feature['datetime'].apply(lambda x: x[:10].replace(\"-\",\"\"))\n",
    "    df_feature['symbol_id'] = df_feature['asset']\n",
    "\n",
    "    df_merge = pd.merge(df_feature,df_label[[\"trade_date\",\"symbol_id\",\"return\"]],how = \"inner\",on = [\"trade_date\",\"symbol_id\"])\n",
    "    df_merge = df_merge.drop(columns = [\"trade_date\",\"symbol_id\"]).reset_index(drop = True)\n",
    "    df_merge = df_merge.sort_values(by=['datetime','asset'])\n",
    "\n",
    "    return df_merge\n",
    "\n",
    "def split_index_feature_label(df_merge) -> tuple:\n",
    "    \"\"\"Split the merged data into three parts: pa_index,pa_feature,pa_label. whose indexs are matched\"\"\"\n",
    "    \n",
    "    df_merge = df_merge.sort_values(['datetime','asset'])\n",
    "\n",
    "    data_index = ProcessArray(df_merge.iloc[:,:2])\n",
    "    data_feature = ProcessArray(df_merge.iloc[:,2:-1])\n",
    "    data_label = ProcessArray(df_merge.iloc[:,-1])\n",
    "    data_column = ProcessArray(df_merge.columns)\n",
    "\n",
    "    return data_index, data_feature, data_label, data_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qianshuofu/anaconda3/envs/rapids-22.12/lib/python3.8/site-packages/sklearn/preprocessing/_data.py:240: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    path_dict = {\"asset_pool_path\":\"/sda/intern_data_shuofu/industry_1070.csv\",\n",
    "                \"feature_path\": \"/home/qianshuofu/factor_qianshuofu/Data/30_minutes_data.feather\",\n",
    "                \"label_path\": \"/home/qianshuofu/factor_qianshuofu/Data/adj_prices.feather\",\n",
    "\n",
    "                \"index_save_path\": \"/home/qianshuofu/factor_qianshuofu/Data/data_index.npy\",\n",
    "                \"feature_save_path\": \"/home/qianshuofu/factor_qianshuofu/Data/data_feature.npy\",\n",
    "                \"label_save_path\": \"/home/qianshuofu/factor_qianshuofu/Data/data_label.npy\",\n",
    "                \"column_save_path\":\"/home/qianshuofu/factor_qianshuofu/Data/data_column.npy\"}\n",
    "\n",
    "    df_asset_pool = pd.read_csv(path_dict['asset_pool_path'])\n",
    "    df_feature = FeatureDataFrame(pd.read_feather(path_dict[\"feature_path\"])).filter_asset(df_asset_pool).change_shape(unstack=True) #.drop_samevalue()\n",
    "    df_label = LabelDataFrame(pd.read_feather(path_dict['label_path'])).get_return()\n",
    "    df_merge = merge_feature_label(df_feature.data,df_label.data)\n",
    "\n",
    "    data_index,data_feature,data_label,data_column = split_index_feature_label(df_merge)\n",
    "    data_feature.fill_na().get_zscore(limit_extreme_value=False)\n",
    "    data_label.get_zscore(limit_extreme_value=False)\n",
    "\n",
    "    np.save(path_dict['index_save_path'],data_index.array)\n",
    "    np.save(path_dict['feature_save_path'],data_feature.array)\n",
    "    np.save(path_dict['label_save_path'],data_label.array)\n",
    "    np.save(path_dict['column_save_path'],data_column.array)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.load('/home/qianshuofu/factor_qianshuofu/Data/data_feature.npy')\n",
    "y = np.load(\"/home/qianshuofu/factor_qianshuofu/Data/data_label.npy\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapids-22.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "f1569305fd3e5ad2ba6ad63ff50c7d3504611b3812e387f42aa745a19d1205bc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
