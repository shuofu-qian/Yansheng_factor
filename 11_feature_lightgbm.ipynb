{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In a tree model, some features whose importances are larger than 0 are used to split the data,\n",
    "# while other features are not. But this does not mean those features not in use are bad absolutly,\n",
    "# we can only say they are less useful and contain less information compared with those factors in use.\n",
    "# Many reasons account for this, including limited tree depth which not allows us to use all features.\n",
    "\n",
    "# So how to exploit all the features' information? Here is an UNPROVEN idea:\n",
    "# A boosting model based on building a tree model to split features according to their importance\n",
    "\n",
    "# In a tree model, we usually split data according to the features, but here we split features according to theri importance, for example:\n",
    "# In the beginning, we have 10 features and we bulid a tree model(maybe a basic tree model or a lightgbm model or others), assuming in this tree model, \n",
    "# The using time for 10 features is w0 = [9,8,7,4,2,1,0,0,0,0], which can used to measure their importance.\n",
    "# Then we split 10 features into 2 groups according whether their using time is larger than 0. \n",
    "# So in this case, we get [f1,f2,f3,f4,f5,f6] in group 1 and [f7,f8,f9,f10] in group 2.\n",
    "# We build a tree model for each feature group and repeat above operations.\n",
    "# This process will stop in limited steps for we will stop split this node if the nums of features in it is 1 or all features are used in a tree model built by them.\n",
    "# Suppose finally we split 10 features into 6 groups, for example: [f1,f2,f3],[f4,f5],[f6,f7],[f8],[f9],[f10], and we get a tree model for each group.\n",
    "# Our final model is a weighted mean of those 6 models. The weight for ith feature is defined as:\n",
    "#                       (w0i+1) / \\Sum(w0k+1) for k from 1 to 10\n",
    "# and the weight for the model built by group i is equal to the sum of weight of the features in this group.\n",
    "# So far we have finished all steps and this ipynb is to test whether this method is useful to improve the model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A boosting model for regression is most likely to get a prediction varying in a narrow interval, for example: the prediction for y is almost near 0.\n",
    "# for the essence of boosting is to average a basket of models and when using average, the variation will be smaller.\n",
    "# So useing MSE to measure whether the model is imporved when boosting them is not reasonable.\n",
    "# Maybe comparing IC is a better way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qianshuofu/anaconda3/envs/rapids-22.12/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import lightgbm\n",
    "from mypython import PurgedGroupTimeSeriesSplit as tss\n",
    "\n",
    "import time\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "x = np.load('/home/qianshuofu/factor_qianshuofu/Data/data_feature.npy')\n",
    "y = np.load('/home/qianshuofu/factor_qianshuofu/Data/data_label.npy')\n",
    "groups = np.load('/home/qianshuofu/factor_qianshuofu/Data/data_index.npy',allow_pickle=True)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "k-fold cross-validation requires at least one train/test split by setting n_splits=2 or more, got n_splits=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m cv \u001b[39m=\u001b[39m tss\u001b[39m.\u001b[39;49mPurgedGroupTimeSeriesSplit(n_splits\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\u001b[39m.\u001b[39msplit_2(x,y,groups)\n\u001b[1;32m      2\u001b[0m cv\n",
      "File \u001b[0;32m~/anaconda3/envs/rapids-22.12/lib/python3.8/site-packages/sklearn/utils/validation.py:71\u001b[0m, in \u001b[0;36m_deprecate_positional_args.<locals>._inner_deprecate_positional_args.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     69\u001b[0m extra_args \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(args) \u001b[39m-\u001b[39m \u001b[39mlen\u001b[39m(all_args)\n\u001b[1;32m     70\u001b[0m \u001b[39mif\u001b[39;00m extra_args \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m---> 71\u001b[0m     \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     73\u001b[0m \u001b[39m# extra_args > 0\u001b[39;00m\n\u001b[1;32m     74\u001b[0m args_msg \u001b[39m=\u001b[39m [\n\u001b[1;32m     75\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(name, arg)\n\u001b[1;32m     76\u001b[0m     \u001b[39mfor\u001b[39;00m name, arg \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(kwonly_args[:extra_args], args[\u001b[39m-\u001b[39mextra_args:])\n\u001b[1;32m     77\u001b[0m ]\n",
      "File \u001b[0;32m~/factor_qianshuofu/Yansheng_factor/mypython/PurgedGroupTimeSeriesSplit.py:44\u001b[0m, in \u001b[0;36mPurgedGroupTimeSeriesSplit.__init__\u001b[0;34m(self, n_splits, max_train_group_size, max_test_group_size, group_gap, verbose)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[39m@_deprecate_positional_args\u001b[39m\n\u001b[1;32m     36\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m,\n\u001b[1;32m     37\u001b[0m              n_splits\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     42\u001b[0m              verbose\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m\n\u001b[1;32m     43\u001b[0m              ):\n\u001b[0;32m---> 44\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(n_splits, shuffle\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, random_state\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m     45\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_train_group_size \u001b[39m=\u001b[39m max_train_group_size\n\u001b[1;32m     46\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroup_gap \u001b[39m=\u001b[39m group_gap\n",
      "File \u001b[0;32m~/anaconda3/envs/rapids-22.12/lib/python3.8/site-packages/sklearn/model_selection/_split.py:298\u001b[0m, in \u001b[0;36m_BaseKFold.__init__\u001b[0;34m(self, n_splits, shuffle, random_state)\u001b[0m\n\u001b[1;32m    295\u001b[0m n_splits \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(n_splits)\n\u001b[1;32m    297\u001b[0m \u001b[39mif\u001b[39;00m n_splits \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m--> 298\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    299\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mk-fold cross-validation requires at least one\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    300\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m train/test split by setting n_splits=2 or more,\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    301\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m got n_splits=\u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(n_splits)\n\u001b[1;32m    302\u001b[0m     )\n\u001b[1;32m    304\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(shuffle, \u001b[39mbool\u001b[39m):\n\u001b[1;32m    305\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mshuffle must be True or False; got \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(shuffle))\n",
      "\u001b[0;31mValueError\u001b[0m: k-fold cross-validation requires at least one train/test split by setting n_splits=2 or more, got n_splits=1."
     ]
    }
   ],
   "source": [
    "cv = tss.PurgedGroupTimeSeriesSplit(n_splits=1).split_2(x,y,groups)\n",
    "cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapids-22.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f1569305fd3e5ad2ba6ad63ff50c7d3504611b3812e387f42aa745a19d1205bc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
