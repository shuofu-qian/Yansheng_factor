{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mypython.MLmodel import * \n",
    "from mypython.Linearmodel import *\n",
    "from mypython.Treemodel import *\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cuml\n",
    "import sklearn\n",
    "import lightgbm\n",
    "import xgboost\n",
    "\n",
    "x = np.load('/home/qianshuofu/factor_qianshuofu/Data/data_feature.npy')\n",
    "y = np.load(\"/home/qianshuofu/factor_qianshuofu/Data/data_label.npy\")\n",
    "# y = np.load(\"/home/qianshuofu/factor_qianshuofu/Data/data_label_scale.npy\")\n",
    "\n",
    "# model_linear = MLmodel(x,y,MyLinear_Gpu(),name='linear')\n",
    "# model_lasso = MLmodel(x,y,MyLasso_Gpu(),name='lasso')\n",
    "# model_ridge = MLmodel(x,y,MyRidge_Gpu(),name='ridge')\n",
    "# model_ElasticNet = MLmodel(x,y,MyElasticNet_Gpu(),name='elasticnet')\n",
    "\n",
    "# model_DT = MLmodel(x,y,MyDTRegressor(),name='DT')\n",
    "# model_RF = MLmodel(x,y,MyRFRegressor_Gpu(),name='RF')\n",
    "# model_LGBM = MLmodel(x,y,MyLGBMRegressor(),name='LGBM')\n",
    "# model_XGB = MLmodel(x,y,MyXGBRegressor(),name='XGB')\n",
    "\n",
    "# def main():\n",
    "#     model_dict = {'linear': MyLinear_Gpu(),\n",
    "#                 'lasso': MyLasso_Gpu(),\n",
    "#                 'ridge': MyRidge_Gpu(),\n",
    "#                 'elasticnet': MyElasticNet_Gpu(),\n",
    "#                 # 'DT': MyDTRegressor(),     # 太慢了跑不出来\n",
    "#                 'RF': MyRFRegressor_Gpu(),  # gpu加速也很慢  7min\n",
    "#                 'LGBM': MyLGBMRegressor(),  # 不使用gpu时,lgbm原生接口和lightgbm.sklearn接口速度一致,45s\n",
    "#                 'XGB': MyXGBRegressor() }   # 不使用gpu时,xgboost接口和xgboost.sklearn接口速度一致,6min40s,慢死了...\n",
    "\n",
    "#     for name,model in model_dict.items():\n",
    "#         MLmodel(x,y,model,name).split_train_test(random_state=42).fit_model().predict().evaluate_model()\n",
    "\n",
    "# main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qianshuofu/anaconda3/envs/rapids-22.12/lib/python3.8/site-packages/lightgbm/sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\tvalid_0's l2: 0.000635311\tvalid_0's l1: 0.0147704\n",
      "[2]\tvalid_0's l2: 0.000634422\tvalid_0's l1: 0.0147609\n",
      "[3]\tvalid_0's l2: 0.0006337\tvalid_0's l1: 0.0147542\n",
      "[4]\tvalid_0's l2: 0.000632986\tvalid_0's l1: 0.014748\n",
      "[5]\tvalid_0's l2: 0.000632434\tvalid_0's l1: 0.0147443\n",
      "[6]\tvalid_0's l2: 0.00063193\tvalid_0's l1: 0.0147399\n",
      "[7]\tvalid_0's l2: 0.000631498\tvalid_0's l1: 0.0147357\n",
      "[8]\tvalid_0's l2: 0.000631177\tvalid_0's l1: 0.0147309\n",
      "[9]\tvalid_0's l2: 0.000630724\tvalid_0's l1: 0.0147269\n",
      "[10]\tvalid_0's l2: 0.000630443\tvalid_0's l1: 0.0147245\n",
      "[11]\tvalid_0's l2: 0.000630084\tvalid_0's l1: 0.014721\n",
      "[12]\tvalid_0's l2: 0.00062987\tvalid_0's l1: 0.0147185\n",
      "[13]\tvalid_0's l2: 0.000629499\tvalid_0's l1: 0.0147156\n",
      "[14]\tvalid_0's l2: 0.00062926\tvalid_0's l1: 0.0147147\n",
      "[15]\tvalid_0's l2: 0.000629049\tvalid_0's l1: 0.0147134\n",
      "[16]\tvalid_0's l2: 0.000628757\tvalid_0's l1: 0.014711\n",
      "[17]\tvalid_0's l2: 0.000628631\tvalid_0's l1: 0.0147098\n",
      "[18]\tvalid_0's l2: 0.000628386\tvalid_0's l1: 0.0147087\n",
      "[19]\tvalid_0's l2: 0.0006282\tvalid_0's l1: 0.0147074\n",
      "[20]\tvalid_0's l2: 0.000628009\tvalid_0's l1: 0.0147057\n",
      "[21]\tvalid_0's l2: 0.000627891\tvalid_0's l1: 0.014705\n",
      "[22]\tvalid_0's l2: 0.000627788\tvalid_0's l1: 0.0147051\n",
      "[23]\tvalid_0's l2: 0.000627628\tvalid_0's l1: 0.0147034\n",
      "[24]\tvalid_0's l2: 0.000627301\tvalid_0's l1: 0.0147003\n",
      "[25]\tvalid_0's l2: 0.000627224\tvalid_0's l1: 0.0147002\n",
      "[26]\tvalid_0's l2: 0.000627101\tvalid_0's l1: 0.0146993\n",
      "[27]\tvalid_0's l2: 0.000626872\tvalid_0's l1: 0.0146974\n",
      "[28]\tvalid_0's l2: 0.000626756\tvalid_0's l1: 0.0146969\n",
      "[29]\tvalid_0's l2: 0.000626674\tvalid_0's l1: 0.0146959\n",
      "[30]\tvalid_0's l2: 0.000626494\tvalid_0's l1: 0.0146948\n",
      "[31]\tvalid_0's l2: 0.000626398\tvalid_0's l1: 0.0146939\n",
      "[32]\tvalid_0's l2: 0.000626176\tvalid_0's l1: 0.0146918\n",
      "[33]\tvalid_0's l2: 0.000625901\tvalid_0's l1: 0.0146893\n",
      "[34]\tvalid_0's l2: 0.000625776\tvalid_0's l1: 0.0146885\n",
      "[35]\tvalid_0's l2: 0.00062569\tvalid_0's l1: 0.0146869\n",
      "[36]\tvalid_0's l2: 0.000625628\tvalid_0's l1: 0.0146855\n",
      "[37]\tvalid_0's l2: 0.00062554\tvalid_0's l1: 0.0146844\n",
      "[38]\tvalid_0's l2: 0.00062528\tvalid_0's l1: 0.014685\n",
      "[39]\tvalid_0's l2: 0.000625122\tvalid_0's l1: 0.0146842\n",
      "[40]\tvalid_0's l2: 0.000625106\tvalid_0's l1: 0.0146841\n",
      "[41]\tvalid_0's l2: 0.000624978\tvalid_0's l1: 0.0146837\n",
      "[42]\tvalid_0's l2: 0.000624872\tvalid_0's l1: 0.0146828\n",
      "[43]\tvalid_0's l2: 0.000624745\tvalid_0's l1: 0.0146819\n",
      "[44]\tvalid_0's l2: 0.000624656\tvalid_0's l1: 0.014681\n",
      "[45]\tvalid_0's l2: 0.000624624\tvalid_0's l1: 0.0146807\n",
      "[46]\tvalid_0's l2: 0.000624538\tvalid_0's l1: 0.0146801\n",
      "[47]\tvalid_0's l2: 0.000624441\tvalid_0's l1: 0.0146792\n",
      "[48]\tvalid_0's l2: 0.000624316\tvalid_0's l1: 0.0146789\n",
      "[49]\tvalid_0's l2: 0.000624292\tvalid_0's l1: 0.0146792\n",
      "[50]\tvalid_0's l2: 0.000624268\tvalid_0's l1: 0.0146782\n",
      "[51]\tvalid_0's l2: 0.000624223\tvalid_0's l1: 0.0146776\n",
      "[52]\tvalid_0's l2: 0.000624256\tvalid_0's l1: 0.0146779\n",
      "[53]\tvalid_0's l2: 0.000624256\tvalid_0's l1: 0.0146777\n",
      "[54]\tvalid_0's l2: 0.000624233\tvalid_0's l1: 0.0146772\n",
      "[55]\tvalid_0's l2: 0.000624038\tvalid_0's l1: 0.0146757\n",
      "[56]\tvalid_0's l2: 0.000623909\tvalid_0's l1: 0.0146743\n",
      "[57]\tvalid_0's l2: 0.000623961\tvalid_0's l1: 0.014675\n",
      "[58]\tvalid_0's l2: 0.000623907\tvalid_0's l1: 0.0146741\n",
      "[59]\tvalid_0's l2: 0.000623889\tvalid_0's l1: 0.0146738\n",
      "[60]\tvalid_0's l2: 0.000623892\tvalid_0's l1: 0.0146742\n",
      "[61]\tvalid_0's l2: 0.000623658\tvalid_0's l1: 0.0146724\n",
      "[62]\tvalid_0's l2: 0.000623575\tvalid_0's l1: 0.0146722\n",
      "[63]\tvalid_0's l2: 0.000623522\tvalid_0's l1: 0.0146714\n",
      "[64]\tvalid_0's l2: 0.000623515\tvalid_0's l1: 0.0146716\n",
      "[65]\tvalid_0's l2: 0.000623443\tvalid_0's l1: 0.0146702\n",
      "[66]\tvalid_0's l2: 0.0006234\tvalid_0's l1: 0.0146698\n",
      "[67]\tvalid_0's l2: 0.000623406\tvalid_0's l1: 0.0146707\n",
      "[68]\tvalid_0's l2: 0.000623365\tvalid_0's l1: 0.01467\n",
      "[69]\tvalid_0's l2: 0.000623214\tvalid_0's l1: 0.0146688\n",
      "[70]\tvalid_0's l2: 0.000623243\tvalid_0's l1: 0.0146697\n",
      "[71]\tvalid_0's l2: 0.00062317\tvalid_0's l1: 0.0146686\n",
      "[72]\tvalid_0's l2: 0.000623012\tvalid_0's l1: 0.0146687\n",
      "[73]\tvalid_0's l2: 0.000623046\tvalid_0's l1: 0.0146693\n",
      "[74]\tvalid_0's l2: 0.000623015\tvalid_0's l1: 0.0146691\n",
      "[75]\tvalid_0's l2: 0.00062295\tvalid_0's l1: 0.014668\n",
      "[76]\tvalid_0's l2: 0.000622932\tvalid_0's l1: 0.0146676\n",
      "[77]\tvalid_0's l2: 0.000623017\tvalid_0's l1: 0.014668\n",
      "[78]\tvalid_0's l2: 0.000622999\tvalid_0's l1: 0.0146681\n",
      "[79]\tvalid_0's l2: 0.000622948\tvalid_0's l1: 0.0146679\n",
      "[80]\tvalid_0's l2: 0.000622776\tvalid_0's l1: 0.0146657\n",
      "[81]\tvalid_0's l2: 0.000622688\tvalid_0's l1: 0.0146648\n",
      "[82]\tvalid_0's l2: 0.000622743\tvalid_0's l1: 0.0146656\n",
      "[83]\tvalid_0's l2: 0.00062276\tvalid_0's l1: 0.0146654\n",
      "[84]\tvalid_0's l2: 0.000622728\tvalid_0's l1: 0.0146652\n",
      "[85]\tvalid_0's l2: 0.00062269\tvalid_0's l1: 0.0146649\n",
      "[86]\tvalid_0's l2: 0.00062269\tvalid_0's l1: 0.0146652\n",
      "[87]\tvalid_0's l2: 0.000622599\tvalid_0's l1: 0.0146641\n",
      "[88]\tvalid_0's l2: 0.000622536\tvalid_0's l1: 0.0146635\n",
      "[89]\tvalid_0's l2: 0.000622522\tvalid_0's l1: 0.014664\n",
      "[90]\tvalid_0's l2: 0.00062249\tvalid_0's l1: 0.014664\n",
      "[91]\tvalid_0's l2: 0.000622541\tvalid_0's l1: 0.0146642\n",
      "[92]\tvalid_0's l2: 0.000622528\tvalid_0's l1: 0.0146642\n",
      "[93]\tvalid_0's l2: 0.000622565\tvalid_0's l1: 0.0146641\n",
      "[94]\tvalid_0's l2: 0.000622627\tvalid_0's l1: 0.0146647\n",
      "[95]\tvalid_0's l2: 0.000622673\tvalid_0's l1: 0.0146648\n",
      "[96]\tvalid_0's l2: 0.000622621\tvalid_0's l1: 0.0146641\n",
      "[97]\tvalid_0's l2: 0.000622614\tvalid_0's l1: 0.0146649\n",
      "[98]\tvalid_0's l2: 0.000622545\tvalid_0's l1: 0.0146645\n",
      "[99]\tvalid_0's l2: 0.000622461\tvalid_0's l1: 0.0146639\n",
      "[100]\tvalid_0's l2: 0.000622418\tvalid_0's l1: 0.014664\n",
      "[101]\tvalid_0's l2: 0.000622299\tvalid_0's l1: 0.0146629\n",
      "[102]\tvalid_0's l2: 0.000622243\tvalid_0's l1: 0.0146621\n",
      "[103]\tvalid_0's l2: 0.000622294\tvalid_0's l1: 0.0146624\n",
      "[104]\tvalid_0's l2: 0.000622177\tvalid_0's l1: 0.0146615\n",
      "[105]\tvalid_0's l2: 0.00062213\tvalid_0's l1: 0.0146614\n",
      "[106]\tvalid_0's l2: 0.00062216\tvalid_0's l1: 0.0146617\n",
      "[107]\tvalid_0's l2: 0.000622103\tvalid_0's l1: 0.014661\n",
      "[108]\tvalid_0's l2: 0.000622133\tvalid_0's l1: 0.0146611\n",
      "[109]\tvalid_0's l2: 0.000622105\tvalid_0's l1: 0.0146614\n",
      "[110]\tvalid_0's l2: 0.000622138\tvalid_0's l1: 0.0146612\n",
      "[111]\tvalid_0's l2: 0.000622167\tvalid_0's l1: 0.0146612\n",
      "[112]\tvalid_0's l2: 0.000622175\tvalid_0's l1: 0.0146616\n",
      "[113]\tvalid_0's l2: 0.000622166\tvalid_0's l1: 0.014662\n",
      "[114]\tvalid_0's l2: 0.000622124\tvalid_0's l1: 0.0146622\n",
      "[115]\tvalid_0's l2: 0.000622147\tvalid_0's l1: 0.0146626\n",
      "[116]\tvalid_0's l2: 0.000622141\tvalid_0's l1: 0.0146629\n",
      "[117]\tvalid_0's l2: 0.00062211\tvalid_0's l1: 0.0146631\n",
      "[118]\tvalid_0's l2: 0.00062208\tvalid_0's l1: 0.0146627\n",
      "[119]\tvalid_0's l2: 0.000621976\tvalid_0's l1: 0.0146618\n",
      "[120]\tvalid_0's l2: 0.000621922\tvalid_0's l1: 0.0146608\n",
      "[121]\tvalid_0's l2: 0.000621871\tvalid_0's l1: 0.0146602\n",
      "[122]\tvalid_0's l2: 0.00062189\tvalid_0's l1: 0.0146604\n",
      "[123]\tvalid_0's l2: 0.000621918\tvalid_0's l1: 0.0146611\n",
      "[124]\tvalid_0's l2: 0.000621912\tvalid_0's l1: 0.0146611\n",
      "[125]\tvalid_0's l2: 0.000621888\tvalid_0's l1: 0.0146611\n",
      "[126]\tvalid_0's l2: 0.000621707\tvalid_0's l1: 0.0146613\n",
      "[127]\tvalid_0's l2: 0.000621636\tvalid_0's l1: 0.0146605\n",
      "[128]\tvalid_0's l2: 0.000621624\tvalid_0's l1: 0.0146606\n",
      "[129]\tvalid_0's l2: 0.000621625\tvalid_0's l1: 0.0146606\n",
      "[130]\tvalid_0's l2: 0.000621623\tvalid_0's l1: 0.0146605\n",
      "[131]\tvalid_0's l2: 0.000621638\tvalid_0's l1: 0.0146604\n",
      "[132]\tvalid_0's l2: 0.00062157\tvalid_0's l1: 0.0146604\n",
      "[133]\tvalid_0's l2: 0.000621513\tvalid_0's l1: 0.0146602\n",
      "[134]\tvalid_0's l2: 0.000621505\tvalid_0's l1: 0.0146604\n",
      "[135]\tvalid_0's l2: 0.000621508\tvalid_0's l1: 0.0146604\n",
      "[136]\tvalid_0's l2: 0.000621413\tvalid_0's l1: 0.0146594\n",
      "[137]\tvalid_0's l2: 0.000621384\tvalid_0's l1: 0.014659\n",
      "[138]\tvalid_0's l2: 0.00062138\tvalid_0's l1: 0.0146589\n",
      "[139]\tvalid_0's l2: 0.000621355\tvalid_0's l1: 0.0146592\n",
      "[140]\tvalid_0's l2: 0.000621348\tvalid_0's l1: 0.0146591\n",
      "[141]\tvalid_0's l2: 0.000621346\tvalid_0's l1: 0.0146593\n",
      "[142]\tvalid_0's l2: 0.000621218\tvalid_0's l1: 0.0146579\n",
      "[143]\tvalid_0's l2: 0.000621182\tvalid_0's l1: 0.0146571\n",
      "[144]\tvalid_0's l2: 0.000621176\tvalid_0's l1: 0.0146566\n",
      "[145]\tvalid_0's l2: 0.000621159\tvalid_0's l1: 0.0146562\n",
      "[146]\tvalid_0's l2: 0.000621089\tvalid_0's l1: 0.0146556\n",
      "[147]\tvalid_0's l2: 0.000621039\tvalid_0's l1: 0.0146556\n",
      "[148]\tvalid_0's l2: 0.000621003\tvalid_0's l1: 0.0146554\n",
      "[149]\tvalid_0's l2: 0.000621023\tvalid_0's l1: 0.0146551\n",
      "[150]\tvalid_0's l2: 0.000621121\tvalid_0's l1: 0.0146562\n",
      "[151]\tvalid_0's l2: 0.000621139\tvalid_0's l1: 0.014657\n",
      "[152]\tvalid_0's l2: 0.000621134\tvalid_0's l1: 0.014657\n",
      "[153]\tvalid_0's l2: 0.000621065\tvalid_0's l1: 0.0146563\n",
      "[154]\tvalid_0's l2: 0.000621038\tvalid_0's l1: 0.0146562\n",
      "[155]\tvalid_0's l2: 0.000621086\tvalid_0's l1: 0.0146568\n",
      "[156]\tvalid_0's l2: 0.000621061\tvalid_0's l1: 0.0146564\n",
      "[157]\tvalid_0's l2: 0.000621089\tvalid_0's l1: 0.0146567\n",
      "[158]\tvalid_0's l2: 0.000621085\tvalid_0's l1: 0.0146567\n",
      "[159]\tvalid_0's l2: 0.000621076\tvalid_0's l1: 0.0146567\n",
      "[160]\tvalid_0's l2: 0.000621066\tvalid_0's l1: 0.0146567\n",
      "[161]\tvalid_0's l2: 0.000621035\tvalid_0's l1: 0.0146566\n",
      "[162]\tvalid_0's l2: 0.000621016\tvalid_0's l1: 0.0146562\n",
      "[163]\tvalid_0's l2: 0.000621011\tvalid_0's l1: 0.0146562\n",
      "[164]\tvalid_0's l2: 0.000620999\tvalid_0's l1: 0.0146561\n",
      "[165]\tvalid_0's l2: 0.000620972\tvalid_0's l1: 0.0146566\n",
      "[166]\tvalid_0's l2: 0.000620914\tvalid_0's l1: 0.0146556\n",
      "[167]\tvalid_0's l2: 0.000620922\tvalid_0's l1: 0.0146557\n",
      "[168]\tvalid_0's l2: 0.000620901\tvalid_0's l1: 0.0146553\n",
      "[169]\tvalid_0's l2: 0.000620856\tvalid_0's l1: 0.014655\n",
      "[170]\tvalid_0's l2: 0.000620858\tvalid_0's l1: 0.0146554\n",
      "[171]\tvalid_0's l2: 0.000620868\tvalid_0's l1: 0.0146556\n",
      "[172]\tvalid_0's l2: 0.000620874\tvalid_0's l1: 0.0146558\n",
      "[173]\tvalid_0's l2: 0.000620858\tvalid_0's l1: 0.0146557\n",
      "[174]\tvalid_0's l2: 0.000620837\tvalid_0's l1: 0.0146557\n",
      "[175]\tvalid_0's l2: 0.000620885\tvalid_0's l1: 0.014656\n",
      "[176]\tvalid_0's l2: 0.000620832\tvalid_0's l1: 0.0146558\n",
      "[177]\tvalid_0's l2: 0.000620832\tvalid_0's l1: 0.014656\n",
      "[178]\tvalid_0's l2: 0.000620784\tvalid_0's l1: 0.0146557\n",
      "[179]\tvalid_0's l2: 0.000620642\tvalid_0's l1: 0.0146569\n",
      "[180]\tvalid_0's l2: 0.000620659\tvalid_0's l1: 0.014656\n",
      "[181]\tvalid_0's l2: 0.000620629\tvalid_0's l1: 0.0146561\n",
      "[182]\tvalid_0's l2: 0.000620649\tvalid_0's l1: 0.0146558\n",
      "[183]\tvalid_0's l2: 0.000620694\tvalid_0's l1: 0.0146562\n",
      "[184]\tvalid_0's l2: 0.000620742\tvalid_0's l1: 0.0146567\n",
      "[185]\tvalid_0's l2: 0.000620758\tvalid_0's l1: 0.0146568\n",
      "[186]\tvalid_0's l2: 0.00062075\tvalid_0's l1: 0.0146568\n",
      "[187]\tvalid_0's l2: 0.000620757\tvalid_0's l1: 0.0146568\n",
      "[188]\tvalid_0's l2: 0.000620777\tvalid_0's l1: 0.014657\n",
      "[189]\tvalid_0's l2: 0.000620778\tvalid_0's l1: 0.0146572\n",
      "[190]\tvalid_0's l2: 0.000620714\tvalid_0's l1: 0.0146565\n",
      "[191]\tvalid_0's l2: 0.000620724\tvalid_0's l1: 0.0146568\n",
      "[192]\tvalid_0's l2: 0.000620729\tvalid_0's l1: 0.0146569\n",
      "[193]\tvalid_0's l2: 0.000620702\tvalid_0's l1: 0.0146566\n",
      "[194]\tvalid_0's l2: 0.000620633\tvalid_0's l1: 0.0146562\n",
      "[195]\tvalid_0's l2: 0.000620679\tvalid_0's l1: 0.0146565\n",
      "[196]\tvalid_0's l2: 0.00062071\tvalid_0's l1: 0.0146566\n",
      "[197]\tvalid_0's l2: 0.000620721\tvalid_0's l1: 0.0146567\n",
      "[198]\tvalid_0's l2: 0.000620789\tvalid_0's l1: 0.0146565\n",
      "[199]\tvalid_0's l2: 0.00062078\tvalid_0's l1: 0.0146565\n",
      "[200]\tvalid_0's l2: 0.000620716\tvalid_0's l1: 0.0146559\n",
      "[201]\tvalid_0's l2: 0.000620733\tvalid_0's l1: 0.0146562\n",
      "[202]\tvalid_0's l2: 0.000620727\tvalid_0's l1: 0.0146559\n",
      "[203]\tvalid_0's l2: 0.000620712\tvalid_0's l1: 0.0146556\n",
      "[204]\tvalid_0's l2: 0.000620671\tvalid_0's l1: 0.0146553\n",
      "[205]\tvalid_0's l2: 0.000620684\tvalid_0's l1: 0.0146556\n",
      "[206]\tvalid_0's l2: 0.00062069\tvalid_0's l1: 0.0146558\n",
      "[207]\tvalid_0's l2: 0.000620739\tvalid_0's l1: 0.0146562\n",
      "[208]\tvalid_0's l2: 0.000620755\tvalid_0's l1: 0.0146563\n",
      "[209]\tvalid_0's l2: 0.000620815\tvalid_0's l1: 0.014657\n",
      "[210]\tvalid_0's l2: 0.000620832\tvalid_0's l1: 0.0146571\n",
      "[211]\tvalid_0's l2: 0.000620837\tvalid_0's l1: 0.0146571\n",
      "[212]\tvalid_0's l2: 0.000620798\tvalid_0's l1: 0.0146565\n",
      "[213]\tvalid_0's l2: 0.000620799\tvalid_0's l1: 0.0146565\n",
      "[214]\tvalid_0's l2: 0.000620744\tvalid_0's l1: 0.0146558\n",
      "[215]\tvalid_0's l2: 0.000620687\tvalid_0's l1: 0.0146551\n",
      "[216]\tvalid_0's l2: 0.000620751\tvalid_0's l1: 0.0146737\n",
      "[217]\tvalid_0's l2: 0.000620721\tvalid_0's l1: 0.0146739\n",
      "[218]\tvalid_0's l2: 0.000620683\tvalid_0's l1: 0.0146735\n",
      "[219]\tvalid_0's l2: 0.000620645\tvalid_0's l1: 0.0146736\n",
      "0.0005763526937905951\n",
      "[[1.         0.11750574]\n",
      " [0.11750574 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test,  y_train, y_test  = sklearn.model_selection.train_test_split(x,y,test_size=0.3,random_state=42)\n",
    "x_train, x_valid, y_train, y_valid = sklearn.model_selection.train_test_split(x_train,y_train,test_size=0.1,random_state=42)\n",
    "\n",
    "# model = lightgbm.train({},lightgbm.Dataset(x_train,y_train),valid_sets=lightgbm.Dataset(x_train,y_train),verbose_eval=1)\n",
    "# model = lightgbm.train({},lightgbm.Dataset(x_train,y_train),verbose_eval=1)\n",
    "model = lightgbm.sklearn.LGBMRegressor(n_estimators=500).fit(x_train,y_train,eval_set=[(x_valid,y_valid)],early_stopping_rounds=50, eval_metric=['l2','l1'])\n",
    "\n",
    "y_pred = model.predict(x_test)\n",
    "print(sklearn.metrics.mean_squared_error(y_test,y_pred))\n",
    "print(np.corrcoef(y_pred.argsort().argsort(),y_test.argsort().argsort()))\n",
    "\n",
    "# without scaling: l2: 0.000576, Corr:0.118, std: 0.024**2=0.000576\n",
    "# scaling:         l2: 0.96627,  Corr:0.122, std: 1**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00124641,  0.00086417,  0.0006297 , ..., -0.00061758,\n",
       "       -0.00107229,  0.00165104])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class LGBMRegressor in module lightgbm.sklearn:\n",
      "\n",
      "class LGBMRegressor(sklearn.base.RegressorMixin, LGBMModel)\n",
      " |  LGBMRegressor(boosting_type: str = 'gbdt', num_leaves: int = 31, max_depth: int = -1, learning_rate: float = 0.1, n_estimators: int = 100, subsample_for_bin: int = 200000, objective: Union[str, Callable, NoneType] = None, class_weight: Union[Dict, str, NoneType] = None, min_split_gain: float = 0.0, min_child_weight: float = 0.001, min_child_samples: int = 20, subsample: float = 1.0, subsample_freq: int = 0, colsample_bytree: float = 1.0, reg_alpha: float = 0.0, reg_lambda: float = 0.0, random_state: Union[int, numpy.random.mtrand.RandomState, NoneType] = None, n_jobs: int = -1, silent: Union[bool, str] = 'warn', importance_type: str = 'split', **kwargs)\n",
      " |  \n",
      " |  LightGBM regressor.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      LGBMRegressor\n",
      " |      sklearn.base.RegressorMixin\n",
      " |      LGBMModel\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  fit(self, X, y, sample_weight=None, init_score=None, eval_set=None, eval_names=None, eval_sample_weight=None, eval_init_score=None, eval_metric=None, early_stopping_rounds=None, verbose='warn', feature_name='auto', categorical_feature='auto', callbacks=None, init_model=None)\n",
      " |      Build a gradient boosting model from the training set (X, y).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like or sparse matrix of shape = [n_samples, n_features]\n",
      " |          Input feature matrix.\n",
      " |      y : array-like of shape = [n_samples]\n",
      " |          The target values (class labels in classification, real numbers in regression).\n",
      " |      sample_weight : array-like of shape = [n_samples] or None, optional (default=None)\n",
      " |          Weights of training data.\n",
      " |      init_score : array-like of shape = [n_samples] or shape = [n_samples * n_classes] (for multi-class task) or shape = [n_samples, n_classes] (for multi-class task) or None, optional (default=None)\n",
      " |          Init score of training data.\n",
      " |      eval_set : list or None, optional (default=None)\n",
      " |          A list of (X, y) tuple pairs to use as validation sets.\n",
      " |      eval_names : list of str, or None, optional (default=None)\n",
      " |          Names of eval_set.\n",
      " |      eval_sample_weight : list of array, or None, optional (default=None)\n",
      " |          Weights of eval data.\n",
      " |      eval_init_score : list of array, or None, optional (default=None)\n",
      " |          Init score of eval data.\n",
      " |      eval_metric : str, callable, list or None, optional (default=None)\n",
      " |          If str, it should be a built-in evaluation metric to use.\n",
      " |          If callable, it should be a custom evaluation metric, see note below for more details.\n",
      " |          If list, it can be a list of built-in metrics, a list of custom evaluation metrics, or a mix of both.\n",
      " |          In either case, the ``metric`` from the model parameters will be evaluated and used as well.\n",
      " |          Default: 'l2' for LGBMRegressor, 'logloss' for LGBMClassifier, 'ndcg' for LGBMRanker.\n",
      " |      early_stopping_rounds : int or None, optional (default=None)\n",
      " |          Activates early stopping. The model will train until the validation score stops improving.\n",
      " |          Validation score needs to improve at least every ``early_stopping_rounds`` round(s)\n",
      " |          to continue training.\n",
      " |          Requires at least one validation data and one metric.\n",
      " |          If there's more than one, will check all of them. But the training data is ignored anyway.\n",
      " |          To check only the first metric, set the ``first_metric_only`` parameter to ``True``\n",
      " |          in additional parameters ``**kwargs`` of the model constructor.\n",
      " |      verbose : bool or int, optional (default=True)\n",
      " |          Requires at least one evaluation data.\n",
      " |          If True, the eval metric on the eval set is printed at each boosting stage.\n",
      " |          If int, the eval metric on the eval set is printed at every ``verbose`` boosting stage.\n",
      " |          The last boosting stage or the boosting stage found by using ``early_stopping_rounds`` is also printed.\n",
      " |      \n",
      " |          .. rubric:: Example\n",
      " |      \n",
      " |          With ``verbose`` = 4 and at least one item in ``eval_set``,\n",
      " |          an evaluation metric is printed every 4 (instead of 1) boosting stages.\n",
      " |      \n",
      " |      feature_name : list of str, or 'auto', optional (default='auto')\n",
      " |          Feature names.\n",
      " |          If 'auto' and data is pandas DataFrame, data columns names are used.\n",
      " |      categorical_feature : list of str or int, or 'auto', optional (default='auto')\n",
      " |          Categorical features.\n",
      " |          If list of int, interpreted as indices.\n",
      " |          If list of str, interpreted as feature names (need to specify ``feature_name`` as well).\n",
      " |          If 'auto' and data is pandas DataFrame, pandas unordered categorical columns are used.\n",
      " |          All values in categorical features should be less than int32 max value (2147483647).\n",
      " |          Large values could be memory consuming. Consider using consecutive integers starting from zero.\n",
      " |          All negative values in categorical features will be treated as missing values.\n",
      " |          The output cannot be monotonically constrained with respect to a categorical feature.\n",
      " |      callbacks : list of callable, or None, optional (default=None)\n",
      " |          List of callback functions that are applied at each iteration.\n",
      " |          See Callbacks in Python API for more information.\n",
      " |      init_model : str, pathlib.Path, Booster, LGBMModel or None, optional (default=None)\n",
      " |          Filename of LightGBM model, Booster instance or LGBMModel instance used for continue training.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Returns self.\n",
      " |      \n",
      " |      \n",
      " |      \n",
      " |      Note\n",
      " |      ----\n",
      " |      Custom eval function expects a callable with following signatures:\n",
      " |      ``func(y_true, y_pred)``, ``func(y_true, y_pred, weight)`` or\n",
      " |      ``func(y_true, y_pred, weight, group)``\n",
      " |      and returns (eval_name, eval_result, is_higher_better) or\n",
      " |      list of (eval_name, eval_result, is_higher_better):\n",
      " |      \n",
      " |          y_true : array-like of shape = [n_samples]\n",
      " |              The target values.\n",
      " |          y_pred : array-like of shape = [n_samples] or shape = [n_samples * n_classes] (for multi-class task)\n",
      " |              The predicted values.\n",
      " |              In case of custom ``objective``, predicted values are returned before any transformation,\n",
      " |              e.g. they are raw margin instead of probability of positive class for binary task in this case.\n",
      " |          weight : array-like of shape = [n_samples]\n",
      " |              The weight of samples.\n",
      " |          group : array-like\n",
      " |              Group/query data.\n",
      " |              Only used in the learning-to-rank task.\n",
      " |              sum(group) = n_samples.\n",
      " |              For example, if you have a 100-document dataset with ``group = [10, 20, 40, 10, 10, 10]``, that means that you have 6 groups,\n",
      " |              where the first 10 records are in the first group, records 11-30 are in the second group, records 31-70 are in the third group, etc.\n",
      " |          eval_name : str\n",
      " |              The name of evaluation function (without whitespace).\n",
      " |          eval_result : float\n",
      " |              The eval result.\n",
      " |          is_higher_better : bool\n",
      " |              Is eval result higher better, e.g. AUC is ``is_higher_better``.\n",
      " |      \n",
      " |      For multi-class task, the y_pred is group by class_id first, then group by row_id.\n",
      " |      If you want to get i-th row y_pred in j-th class, the access way is y_pred[j * num_data + i].\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.RegressorMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Return the coefficient of determination of the prediction.\n",
      " |      \n",
      " |      The coefficient of determination :math:`R^2` is defined as\n",
      " |      :math:`(1 - \\frac{u}{v})`, where :math:`u` is the residual\n",
      " |      sum of squares ``((y_true - y_pred)** 2).sum()`` and :math:`v`\n",
      " |      is the total sum of squares ``((y_true - y_true.mean()) ** 2).sum()``.\n",
      " |      The best possible score is 1.0 and it can be negative (because the\n",
      " |      model can be arbitrarily worse). A constant model that always predicts\n",
      " |      the expected value of `y`, disregarding the input features, would get\n",
      " |      a :math:`R^2` score of 0.0.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Test samples. For some estimators this may be a precomputed\n",
      " |          kernel matrix or a list of generic objects instead with shape\n",
      " |          ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n",
      " |          is the number of samples used in the fitting for the estimator.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          True values for `X`.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          :math:`R^2` of ``self.predict(X)`` wrt. `y`.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      The :math:`R^2` score used when calling ``score`` on a regressor uses\n",
      " |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      " |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
      " |      This influences the ``score`` method of all the multioutput\n",
      " |      regressors (except for\n",
      " |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.RegressorMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from LGBMModel:\n",
      " |  \n",
      " |  __init__(self, boosting_type: str = 'gbdt', num_leaves: int = 31, max_depth: int = -1, learning_rate: float = 0.1, n_estimators: int = 100, subsample_for_bin: int = 200000, objective: Union[str, Callable, NoneType] = None, class_weight: Union[Dict, str, NoneType] = None, min_split_gain: float = 0.0, min_child_weight: float = 0.001, min_child_samples: int = 20, subsample: float = 1.0, subsample_freq: int = 0, colsample_bytree: float = 1.0, reg_alpha: float = 0.0, reg_lambda: float = 0.0, random_state: Union[int, numpy.random.mtrand.RandomState, NoneType] = None, n_jobs: int = -1, silent: Union[bool, str] = 'warn', importance_type: str = 'split', **kwargs)\n",
      " |      Construct a gradient boosting model.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      boosting_type : str, optional (default='gbdt')\n",
      " |          'gbdt', traditional Gradient Boosting Decision Tree.\n",
      " |          'dart', Dropouts meet Multiple Additive Regression Trees.\n",
      " |          'goss', Gradient-based One-Side Sampling.\n",
      " |          'rf', Random Forest.\n",
      " |      num_leaves : int, optional (default=31)\n",
      " |          Maximum tree leaves for base learners.\n",
      " |      max_depth : int, optional (default=-1)\n",
      " |          Maximum tree depth for base learners, <=0 means no limit.\n",
      " |      learning_rate : float, optional (default=0.1)\n",
      " |          Boosting learning rate.\n",
      " |          You can use ``callbacks`` parameter of ``fit`` method to shrink/adapt learning rate\n",
      " |          in training using ``reset_parameter`` callback.\n",
      " |          Note, that this will ignore the ``learning_rate`` argument in training.\n",
      " |      n_estimators : int, optional (default=100)\n",
      " |          Number of boosted trees to fit.\n",
      " |      subsample_for_bin : int, optional (default=200000)\n",
      " |          Number of samples for constructing bins.\n",
      " |      objective : str, callable or None, optional (default=None)\n",
      " |          Specify the learning task and the corresponding learning objective or\n",
      " |          a custom objective function to be used (see note below).\n",
      " |          Default: 'regression' for LGBMRegressor, 'binary' or 'multiclass' for LGBMClassifier, 'lambdarank' for LGBMRanker.\n",
      " |      class_weight : dict, 'balanced' or None, optional (default=None)\n",
      " |          Weights associated with classes in the form ``{class_label: weight}``.\n",
      " |          Use this parameter only for multi-class classification task;\n",
      " |          for binary classification task you may use ``is_unbalance`` or ``scale_pos_weight`` parameters.\n",
      " |          Note, that the usage of all these parameters will result in poor estimates of the individual class probabilities.\n",
      " |          You may want to consider performing probability calibration\n",
      " |          (https://scikit-learn.org/stable/modules/calibration.html) of your model.\n",
      " |          The 'balanced' mode uses the values of y to automatically adjust weights\n",
      " |          inversely proportional to class frequencies in the input data as ``n_samples / (n_classes * np.bincount(y))``.\n",
      " |          If None, all classes are supposed to have weight one.\n",
      " |          Note, that these weights will be multiplied with ``sample_weight`` (passed through the ``fit`` method)\n",
      " |          if ``sample_weight`` is specified.\n",
      " |      min_split_gain : float, optional (default=0.)\n",
      " |          Minimum loss reduction required to make a further partition on a leaf node of the tree.\n",
      " |      min_child_weight : float, optional (default=1e-3)\n",
      " |          Minimum sum of instance weight (hessian) needed in a child (leaf).\n",
      " |      min_child_samples : int, optional (default=20)\n",
      " |          Minimum number of data needed in a child (leaf).\n",
      " |      subsample : float, optional (default=1.)\n",
      " |          Subsample ratio of the training instance.\n",
      " |      subsample_freq : int, optional (default=0)\n",
      " |          Frequency of subsample, <=0 means no enable.\n",
      " |      colsample_bytree : float, optional (default=1.)\n",
      " |          Subsample ratio of columns when constructing each tree.\n",
      " |      reg_alpha : float, optional (default=0.)\n",
      " |          L1 regularization term on weights.\n",
      " |      reg_lambda : float, optional (default=0.)\n",
      " |          L2 regularization term on weights.\n",
      " |      random_state : int, RandomState object or None, optional (default=None)\n",
      " |          Random number seed.\n",
      " |          If int, this number is used to seed the C++ code.\n",
      " |          If RandomState object (numpy), a random integer is picked based on its state to seed the C++ code.\n",
      " |          If None, default seeds in C++ code are used.\n",
      " |      n_jobs : int, optional (default=-1)\n",
      " |          Number of parallel threads.\n",
      " |      silent : bool, optional (default=True)\n",
      " |          Whether to print messages while running boosting.\n",
      " |      importance_type : str, optional (default='split')\n",
      " |          The type of feature importance to be filled into ``feature_importances_``.\n",
      " |          If 'split', result contains numbers of times the feature is used in a model.\n",
      " |          If 'gain', result contains total gains of splits which use the feature.\n",
      " |      **kwargs\n",
      " |          Other parameters for the model.\n",
      " |          Check http://lightgbm.readthedocs.io/en/latest/Parameters.html for more parameters.\n",
      " |      \n",
      " |          .. warning::\n",
      " |      \n",
      " |              \\*\\*kwargs is not supported in sklearn, it may cause unexpected issues.\n",
      " |      \n",
      " |      Note\n",
      " |      ----\n",
      " |      A custom objective function can be provided for the ``objective`` parameter.\n",
      " |      In this case, it should have the signature\n",
      " |      ``objective(y_true, y_pred) -> grad, hess`` or\n",
      " |      ``objective(y_true, y_pred, group) -> grad, hess``:\n",
      " |      \n",
      " |          y_true : array-like of shape = [n_samples]\n",
      " |              The target values.\n",
      " |          y_pred : array-like of shape = [n_samples] or shape = [n_samples * n_classes] (for multi-class task)\n",
      " |              The predicted values.\n",
      " |              Predicted values are returned before any transformation,\n",
      " |              e.g. they are raw margin instead of probability of positive class for binary task.\n",
      " |          group : array-like\n",
      " |              Group/query data.\n",
      " |              Only used in the learning-to-rank task.\n",
      " |              sum(group) = n_samples.\n",
      " |              For example, if you have a 100-document dataset with ``group = [10, 20, 40, 10, 10, 10]``, that means that you have 6 groups,\n",
      " |              where the first 10 records are in the first group, records 11-30 are in the second group, records 31-70 are in the third group, etc.\n",
      " |          grad : array-like of shape = [n_samples] or shape = [n_samples * n_classes] (for multi-class task)\n",
      " |              The value of the first order derivative (gradient) of the loss\n",
      " |              with respect to the elements of y_pred for each sample point.\n",
      " |          hess : array-like of shape = [n_samples] or shape = [n_samples * n_classes] (for multi-class task)\n",
      " |              The value of the second order derivative (Hessian) of the loss\n",
      " |              with respect to the elements of y_pred for each sample point.\n",
      " |      \n",
      " |      For multi-class task, the y_pred is group by class_id first, then group by row_id.\n",
      " |      If you want to get i-th row y_pred in j-th class, the access way is y_pred[j * num_data + i]\n",
      " |      and you should group grad and hess in this way as well.\n",
      " |  \n",
      " |  __sklearn_is_fitted__(self) -> bool\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, optional (default=True)\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : dict\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  predict(self, X, raw_score=False, start_iteration=0, num_iteration=None, pred_leaf=False, pred_contrib=False, **kwargs)\n",
      " |      Return the predicted value for each sample.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like or sparse matrix of shape = [n_samples, n_features]\n",
      " |          Input features matrix.\n",
      " |      raw_score : bool, optional (default=False)\n",
      " |          Whether to predict raw scores.\n",
      " |      start_iteration : int, optional (default=0)\n",
      " |          Start index of the iteration to predict.\n",
      " |          If <= 0, starts from the first iteration.\n",
      " |      num_iteration : int or None, optional (default=None)\n",
      " |          Total number of iterations used in the prediction.\n",
      " |          If None, if the best iteration exists and start_iteration <= 0, the best iteration is used;\n",
      " |          otherwise, all iterations from ``start_iteration`` are used (no limits).\n",
      " |          If <= 0, all iterations from ``start_iteration`` are used (no limits).\n",
      " |      pred_leaf : bool, optional (default=False)\n",
      " |          Whether to predict leaf index.\n",
      " |      pred_contrib : bool, optional (default=False)\n",
      " |          Whether to predict feature contributions.\n",
      " |      \n",
      " |          .. note::\n",
      " |      \n",
      " |              If you want to get more explanations for your model's predictions using SHAP values,\n",
      " |              like SHAP interaction values,\n",
      " |              you can install the shap package (https://github.com/slundberg/shap).\n",
      " |              Note that unlike the shap package, with ``pred_contrib`` we return a matrix with an extra\n",
      " |              column, where the last column is the expected value.\n",
      " |      \n",
      " |      **kwargs\n",
      " |          Other parameters for the prediction.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      predicted_result : array-like of shape = [n_samples] or shape = [n_samples, n_classes]\n",
      " |          The predicted values.\n",
      " |      X_leaves : array-like of shape = [n_samples, n_trees] or shape = [n_samples, n_trees * n_classes]\n",
      " |          If ``pred_leaf=True``, the predicted leaf of every tree for each sample.\n",
      " |      X_SHAP_values : array-like of shape = [n_samples, n_features + 1] or shape = [n_samples, (n_features + 1) * n_classes] or list with n_classes length of such objects\n",
      " |          If ``pred_contrib=True``, the feature contributions for each sample.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params\n",
      " |          Parameter names with their new values.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Returns self.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from LGBMModel:\n",
      " |  \n",
      " |  best_iteration_\n",
      " |      :obj:`int` or :obj:`None`: The best iteration of fitted model if ``early_stopping()`` callback has been specified.\n",
      " |  \n",
      " |  best_score_\n",
      " |      :obj:`dict`: The best score of fitted model.\n",
      " |  \n",
      " |  booster_\n",
      " |      Booster: The underlying Booster of this model.\n",
      " |  \n",
      " |  evals_result_\n",
      " |      :obj:`dict` or :obj:`None`: The evaluation results if validation sets have been specified.\n",
      " |  \n",
      " |  feature_importances_\n",
      " |      :obj:`array` of shape = [n_features]: The feature importances (the higher, the more important).\n",
      " |      \n",
      " |      .. note::\n",
      " |      \n",
      " |          ``importance_type`` attribute is passed to the function\n",
      " |          to configure the type of importance values to be extracted.\n",
      " |  \n",
      " |  feature_name_\n",
      " |      :obj:`array` of shape = [n_features]: The names of features.\n",
      " |  \n",
      " |  n_features_\n",
      " |      :obj:`int`: The number of features of fitted model.\n",
      " |  \n",
      " |  n_features_in_\n",
      " |      :obj:`int`: The number of features of fitted model.\n",
      " |  \n",
      " |  objective_\n",
      " |      :obj:`str` or :obj:`callable`: The concrete objective used while fitting this model.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(lightgbm.sklearn.LGBMRegressor)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapids-22.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "f1569305fd3e5ad2ba6ad63ff50c7d3504611b3812e387f42aa745a19d1205bc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
